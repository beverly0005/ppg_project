{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "BhKiJvgw6DK2"
      ],
      "authorship_tag": "ABX9TyNoXSR1FMLxkKdkBp1He/Wh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "End-to-end data processing from data downloading to 1-second data segmentation generation"
      ],
      "metadata": {
        "id": "_BVt9XSP24mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Libraries"
      ],
      "metadata": {
        "id": "qWgNfVcE3CC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vitaldb neurokit2"
      ],
      "metadata": {
        "id": "ZnTLIy4V3Gp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cea702e-75ea-4bd0-c3ce-6c7e0b842a83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vitaldb\n",
            "  Downloading vitaldb-1.5.8-py3-none-any.whl.metadata (314 bytes)\n",
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.2.12-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vitaldb) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from vitaldb) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vitaldb) (2.32.4)\n",
            "Collecting wfdb (from vitaldb)\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (3.10.0)\n",
            "Requirement already satisfied: PyWavelets>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->vitaldb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->vitaldb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vitaldb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vitaldb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vitaldb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vitaldb) (2025.10.5)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb->vitaldb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->vitaldb) (2025.3.0)\n",
            "Collecting pandas (from vitaldb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->vitaldb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->vitaldb) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->neurokit2) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->vitaldb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb->vitaldb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->vitaldb) (2.23)\n",
            "Downloading vitaldb-1.5.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neurokit2-0.2.12-py2.py3-none-any.whl (708 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb, neurokit2, vitaldb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed neurokit2-0.2.12 pandas-2.3.3 vitaldb-1.5.8 wfdb-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import vitaldb\n",
        "import io\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import neurokit2 as nk\n",
        "\n",
        "from scipy import signal\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Qpik1HNX6cgv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehKKAaV-6fyC",
        "outputId": "db8d1ef8-1500-44f3-ee14-1c0e7f7f40d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User-Defined Parameters\n",
        "* All urls used in data downloading\n",
        "* All filenames and paths\n",
        "* All parameters used in data processing"
      ],
      "metadata": {
        "id": "lojJCX-X3LUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAWDATAPATH = 'drive/MyDrive/2025_PPG_GLUC/Data/Raw Data/'\n",
        "VITALDB_DATAPATH = 'drive/MyDrive/2025_PPG_GLUC/Data/Raw Data/VitalDB/'\n",
        "OUTPUT_PATH = 'drive/MyDrive/2025_PPG_GLUC/Data/Processed Data/BW_ppg_16min_windows/'\n",
        "\n",
        "INPUT_PATH = 'drive/MyDrive/2025_PPG_GLUC/Data/Processed Data/BW_ppg_16min_windows/'\n",
        "OUTPUT_PATH2 = 'drive/MyDrive/2025_PPG_GLUC/Data/Processed Data/BW_ppg_16min_filtered/'\n",
        "OUTPUT_PATH3 = 'drive/MyDrive/2025_PPG_GLUC/Data/Final Data/1s_segmentation/' # Final Output\n",
        "\n",
        "# Signal parameters\n",
        "FS = 100  # Sampling frequency (Hz)\n",
        "WINDOW_SIZE = 100  # 1 second = 100 samples\n",
        "\n",
        "# Thresholds from paper\n",
        "HEIGHT_THRESHOLD = 20\n",
        "DISTANCE_THRESHOLD = int(0.8 * FS)  # 0.8s min between peaks\n",
        "SIMILARITY_THRESHOLD = 0.85"
      ],
      "metadata": {
        "id": "UdOyJLz-3Gmm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(RAWDATAPATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ddey8IX6y3Y",
        "outputId": "25c0ccdb-c7fe-476a-d658-ce71b8c2c8aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['clinical_parameters.csv',\n",
              " 'DataSource.txt',\n",
              " 'lab_parameters.csv',\n",
              " 'track_names.csv',\n",
              " 'VitalDB',\n",
              " 'lab_data.csv',\n",
              " 'clinical_data.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data (One-Time)"
      ],
      "metadata": {
        "id": "kXpzdf-w3YTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are discrepancies in clinical data (at least in age) being found between url version and flatfile version\n",
        "\n",
        "Use url version which is close to what paper uses"
      ],
      "metadata": {
        "id": "1vG1zHOw7-8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab and Clinical Data"
      ],
      "metadata": {
        "id": "UOE_F4mX8NEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download Lab Data\n",
        "# labs_url = \"https://api.vitaldb.net/labs\"\n",
        "# labs_response = requests.get(labs_url)\n",
        "\n",
        "# if labs_response.status_code != 200:\n",
        "#     print(f\"Failed to load labs data: {labs_response.status_code}\")\n",
        "# else:\n",
        "#     labs_data = pd.read_csv(io.StringIO(labs_response.text))\n",
        "#     print(f\"Total labs measurements: {len(labs_data)}\")\n",
        "#     labs_data.to_csv(RAWDATAPATH + 'lab_data.csv')\n",
        "\n",
        "# # Download Clinical Data\n",
        "# clinical_url = \"https://api.vitaldb.net/cases\"\n",
        "# clinical_response = requests.get(clinical_url)\n",
        "\n",
        "# if clinical_response.status_code != 200:\n",
        "#     print(f\"Failed to load clinical data: {clinical_response.status_code}\")\n",
        "# else:\n",
        "#     clinical_data = pd.read_csv(io.StringIO(clinical_response.text))\n",
        "#     print(f\"Clinical data loaded: {len(clinical_data)} cases\")\n",
        "#     clinical_data.to_csv(RAWDATAPATH + 'clinical_data.csv')"
      ],
      "metadata": {
        "id": "6nbe7Gu93Gjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VitalDB"
      ],
      "metadata": {
        "id": "tdTy9uNX8Q13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Find Cases with Glucose Lab Data\n",
        "# lab = pd.read_csv(RAWDATAPATH + 'lab_data.csv')\n",
        "# lab_gluc = lab[lab['name'] == 'gluc']\n",
        "# lab_gluc_cases = lab_gluc['caseid'].unique().tolist()\n",
        "\n",
        "# # Obtain PLETH (PPG) Data: Original PPG is 500 Hz, need to downsample to 100 Hz (from paper)\n",
        "# tracklist = ['SNUADC/PLETH']\n",
        "# existlist = [int(i.split('_')[-1].split('.')[0]) for i in os.listdir(VITALDB_DATAPATH + 'Vital/')]\n",
        "# to_retrieve = list(set(lab_gluc_cases) - set(existlist))\n",
        "# for caseid in tqdm(to_retrieve):\n",
        "#     tmp = vitaldb.load_case(caseid, tracklist, interval=1/100)\n",
        "#     np.save(VITALDB_DATAPATH + 'Vital/' + f'vitaldb_PLETH_case_{caseid}.npy', tmp)\n",
        "\n",
        "# # Re-try for those size ~1 KB\n",
        "# tracklist = ['SNUADC/PLETH']\n",
        "# existlist = [int(i.split('_')[-1].split('.')[0]) for i in os.listdir(VITALDB_DATAPATH + 'Vital/')]\n",
        "# original_list = [caseid for caseid in existlist if os.path.getsize(\n",
        "#     VITALDB_DATAPATH + 'Vital/' + f'vitaldb_PLETH_case_{caseid}.npy')/1024 <= 1]\n",
        "# print(len(original_list))\n",
        "\n",
        "# for caseid in tqdm(original_list):\n",
        "#     tmp = vitaldb.load_case(caseid, tracklist, interval=1/100)\n",
        "#     np.save(VITALDB_DATAPATH + 'Vital/' + f'vitaldb_PLETH_case_{caseid}.npy', tmp)\n",
        "\n",
        "# # Check again\n",
        "# after_list = [caseid for caseid in existlist if os.path.getsize(\n",
        "#     VITALDB_DATAPATH + 'Vital/' + f'vitaldb_PLETH_case_{caseid}.npy')/1024 <= 1]\n",
        "# print(len(after_list))\n"
      ],
      "metadata": {
        "id": "4ywVxPun8SUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPG Temporal Alignment\n",
        "* Find Caseids with Glucose Reading\n",
        "* Filter Glucose Reading by Case Timings\n",
        "* Generate PPG alignment"
      ],
      "metadata": {
        "id": "2QHsnHaC3nLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labs_data = pd.read_csv(RAWDATAPATH + 'lab_data.csv').drop(columns=['Unnamed: 0'])\n",
        "clinical_data = pd.read_csv(RAWDATAPATH + 'clinical_data.csv').drop(columns=['Unnamed: 0'])\n",
        "\n",
        "labs_data.shape, clinical_data.shape, clinical_data['caseid'].nunique()"
      ],
      "metadata": {
        "id": "CgE3mA5r3Gg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e05e57-f791-4aa6-f1b6-8fb224881026"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((928448, 4), (6388, 74), 6388)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Caseids with Glucose Reading"
      ],
      "metadata": {
        "id": "HVcUrY0n38kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glucose_data = labs_data[labs_data['name'] == 'gluc'].copy()\n",
        "print(glucose_data.shape, glucose_data['caseid'].nunique())\n",
        "\n",
        "# Glucose reading outside case time\n",
        "print(glucose_data[glucose_data['dt']<0].shape[0]/glucose_data.shape[0])"
      ],
      "metadata": {
        "id": "IBeAaCee3Gej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba57698-54c6-48cb-870a-9b420e55b0e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35358, 4) 5091\n",
            "0.255218055319871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Glucose Reading by Case Timings\n",
        "Filters applied:\n",
        "* dt >= casestart\n",
        "* dt <= caseend"
      ],
      "metadata": {
        "id": "_dgBnYvK4BP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glucose_with_timing = pd.merge(glucose_data,\n",
        "                               clinical_data[['caseid', 'casestart', 'caseend']],\n",
        "                               on='caseid', how='left')\n",
        "print(clinical_data.shape, glucose_with_timing.shape)\n",
        "print(clinical_data[clinical_data['casestart'].isna()].shape,\n",
        "      clinical_data[clinical_data['caseend'].isna()].shape,\n",
        "      glucose_with_timing[glucose_with_timing['dt'].isna()].shape,\n",
        "      glucose_with_timing['caseid'].nunique())\n",
        "\n",
        "# Valid time\n",
        "valid_timing = glucose_with_timing[\n",
        "        (glucose_with_timing['dt'] >= glucose_with_timing['casestart']) &\n",
        "        (glucose_with_timing['dt'] <= glucose_with_timing['caseend'])\n",
        "    ]\n",
        "print(valid_timing.shape, valid_timing['caseid'].nunique())"
      ],
      "metadata": {
        "id": "hxCo_dZx4C-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3156b9a2-a106-4556-93d0-88c944280ad6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6388, 74) (35358, 6)\n",
            "(0, 74) (0, 74) (0, 6) 5091\n",
            "(8847, 6) 3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate PPG Temporal Alignment\n",
        "*   Load PPG data from .npy files by case ID\n",
        "*   Extract 16-minute windows\n",
        "*   Temporal Alignment using timestamps\n",
        "\n",
        "**Temporal Alignment Calculated Using**:\n",
        "* `time_offset = glucose_time - case_start` - Time of glucose measurement relative to case start (in seconds)\n",
        "* `center_sample = int(time_offset * sampling_rate)` - Exact sample index where glucose was measured\n",
        "* `half_window = window_samples // 2` - Half of 16-minute window = 48,000 samples (8 minutes)\n",
        "* `start_idx = center_sample - half_window` - Start of window (8 minutes before glucose measurement)\n",
        "* `end_idx = center_sample + half_window` - End of window (8 minutes after glucose measurement)\n",
        "* `ppg_window = ppg_data[start_idx:end_idx]` - Extract 96,000 samples (16 minutes at 100 Hz)\n",
        "\n",
        "**Which cases are removed / avoided?**:\n",
        "* Any case that the glucose reading is not within the case start / case end is not used\n",
        "* Any case that has the glucose reading too close to the case start / case end where it is not able to extract the full 16 minute window\n",
        "* Any case that has missing PPG data\n",
        "* Any case that has misisng case timing information"
      ],
      "metadata": {
        "id": "1hVdXVBW4O2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort caseid\n",
        "valid_timing = valid_timing.sort_values(by='caseid')\n",
        "\n",
        "# Parameters\n",
        "sampling_rate = 100\n",
        "window_duration = 16 * 60  # seconds\n",
        "window_samples = window_duration * sampling_rate\n",
        "caseid_holder = int(-1)\n",
        "\n",
        "# Generate meta data for tracking purpose\n",
        "ppg_data_meta = pd.DataFrame({'Caseid':[], 'Gluc':[], 'Info':[]})\n",
        "\n",
        "for index, row in tqdm(valid_timing.iterrows()):\n",
        "    caseid = row['caseid']\n",
        "    glucose_time = row['dt']\n",
        "    case_start = row['casestart']\n",
        "    filename = f\"case_{caseid}_time_{glucose_time}_cleaned.npy\"\n",
        "\n",
        "    # Skip if already exist\n",
        "    if os.path.exists(OUTPUT_PATH + filename):\n",
        "        continue\n",
        "\n",
        "    # Load Data\n",
        "    if caseid != caseid_holder:  # Reload\n",
        "        ppg_file = f\"{RAWDATAPATH}VitalDB/Vital/vitaldb_PLETH_case_{caseid}.npy\"\n",
        "        try:\n",
        "            ppg_data = np.load(ppg_file)\n",
        "            caseid_holder = caseid\n",
        "        except:\n",
        "            print(f'Error in loading vitaldb_PLETH_case_{caseid}.npy')\n",
        "            continue  # skip to next\n",
        "\n",
        "    # Extract window\n",
        "    time_offset = glucose_time - case_start # to calculate relative time to case start\n",
        "    center_sample = int(time_offset * sampling_rate) # to find the sampling index for gluc reading\n",
        "    half_window = window_samples // 2 # for the 8 minute window\n",
        "\n",
        "    start_idx = center_sample - half_window # start of window\n",
        "    end_idx = center_sample + half_window # end of window\n",
        "\n",
        "    if start_idx < 0 or end_idx >= len(ppg_data): # if ppg data out of the boundary, returns none\n",
        "        print(f'Error in extracting {caseid}: out of boundary')\n",
        "        continue\n",
        "    else: # Forward filling missing\n",
        "        cleaned_ppg_data = pd.Series(ppg_data[start_idx:end_idx].flatten()).ffill().values\n",
        "        np.save(OUTPUT_PATH + filename, cleaned_ppg_data)\n",
        "\n",
        "    # Note down the missing values that being forward filled\n",
        "    original_nan = sum([1 for i in ppg_data.flatten() if np.isnan(i)])\n",
        "    filled_nan = sum([1 for i in cleaned_ppg_data.flatten() if np.isnan(i)])\n",
        "    ppg_data_meta = pd.concat([ppg_data_meta,\n",
        "        pd.DataFrame({'Caseid':[caseid], 'Gluc': [glucose_time],\n",
        "                      'Info': [f'Original missing: {original_nan}, After filling missing: {filled_nan}']})],\n",
        "        ignore_index=True)\n",
        "\n",
        "ppg_data_meta.to_csv(OUTPUT_PATH + 'BW_ppg_16min_metadata.csv', index=False)\n",
        "ppg_data_meta.shape"
      ],
      "metadata": {
        "id": "L8ctEVKa4Wqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter PPG\n",
        "* Butterworth filter\n",
        "\n",
        "**Nyquist Sampling Theorem**: https://www.geeksforgeeks.org/electronics-engineering/nyquist-sampling-theorem/\n",
        "\n",
        "To accurately capture a signal, your sampling frequency must be at least twice the highest frequency component in that signal.\n",
        "\n",
        "Otherwise, it may have band aliasing, i.e. high-freq signal appears like low-freq signal, causing distortion"
      ],
      "metadata": {
        "id": "efvQgLLZ4k6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "sampling_rate = 100\n",
        "low_cut = 0.5\n",
        "high_cut = 8\n",
        "order = 3\n",
        "\n",
        "# Check the remaining\n",
        "tmp_ppg_data_meta = ppg_data_meta[(~ppg_data_meta['Info'].str.contains('Out of boundary')) & (\n",
        "    ~ppg_data_meta['Info'].str.contains('After filling missing: 96000'))].copy()\n",
        "tmp_ppg_data_meta['filename'] = tmp_ppg_data_meta.apply(\n",
        "    lambda x: f\"case_{int(x['Caseid'])}_time_{int(x['Gluc'])}_filtered.npy\", axis=1)\n",
        "\n",
        "for index, row in tqdm(tmp_ppg_data_meta[~tmp_ppg_data_meta['filename'].isin(\n",
        "    os.listdir(OUTPUT_PATH2))].iterrows()):\n",
        "    if (row['Info'] == 'Out of boundary') | ('After filling missing: 96000' in row['Info']):\n",
        "        continue\n",
        "    else:\n",
        "        caseid = int(row['Caseid'])\n",
        "        glucose_time = int(row['Gluc'])\n",
        "        ppg_file = f\"{INPUT_PATH}case_{caseid}_time_{glucose_time}_cleaned.npy\"\n",
        "        output_file = f\"{OUTPUT_PATH2}case_{caseid}_time_{glucose_time}_filtered.npy\"\n",
        "        try:\n",
        "            ppg_data = np.load(ppg_file)\n",
        "            ppg_filtered = nk.signal_filter(\n",
        "                ppg_data,\n",
        "                sampling_rate=sampling_rate,\n",
        "                lowcut=low_cut,       # low cut frequency (Hz)\n",
        "                highcut=high_cut,        # high cut frequency (Hz)\n",
        "                method='butterworth',\n",
        "                order=order\n",
        "            )\n",
        "            np.save(output_file, ppg_filtered)\n",
        "        except:\n",
        "            print(f'Error in loading {ppg_file}')\n"
      ],
      "metadata": {
        "id": "DPFO7duH4C8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate 1-Second Segmentation\n",
        "* Peak detection on 16 min window to find all systolic peaks\n",
        "* Extract 1s windows on each peak\n",
        "* Create the template (mean of all windows per case)\n",
        "* Perform Cosine similarity filtering (threshold at 0.85)\n",
        "* Height threshold at 20 amplitude\n",
        "* Distance threshold set to 0.8s min between peaks"
      ],
      "metadata": {
        "id": "Psk3IE3_5CFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all filtered files\n",
        "ppg_files = sorted(Path(OUTPUT_PATH2).glob('case_*_filtered.npy'))\n",
        "print(f\"Found {len(ppg_files)} files to process\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ9HQ53QB2CJ",
        "outputId": "b9e1ab6d-6aeb-445c-8962-3e75f53d6f3d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7300 files to process\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Mini-Functions"
      ],
      "metadata": {
        "id": "BhKiJvgw6DK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using scipy's find_peaks, prominence set to 0"
      ],
      "metadata": {
        "id": "VfeRLX8fCTkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Peak Detection"
      ],
      "metadata": {
        "id": "52Qc5DJs5W2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_peaks(ppg_signal, height_threshold=HEIGHT_THRESHOLD,\n",
        "                 distance_threshold=DISTANCE_THRESHOLD, sampling_rate=FS):\n",
        "    peaks, _ = find_peaks(\n",
        "        ppg_signal,\n",
        "        height=height_threshold,\n",
        "        distance=distance_threshold\n",
        "    )\n",
        "\n",
        "    return peaks"
      ],
      "metadata": {
        "id": "9HbqgBmd3GcM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Windows\n",
        "- Extracts 1s windows centered on each peak\n",
        "- Only keep windows that contain exactly 1 peak\n",
        "- window_size set to 100 samples (1s)\n",
        "- Returns: list of valid windows (windows)"
      ],
      "metadata": {
        "id": "t_bioHMF5cmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_windows(ppg_signal, peaks, window_size):\n",
        "    windows = []\n",
        "    half_window = window_size // 2  # 50 samples\n",
        "\n",
        "    for peak in peaks:\n",
        "        # Calculate window boundaries\n",
        "        window_start = max(0, peak - half_window)\n",
        "        window_end = min(len(ppg_signal), peak + half_window)\n",
        "\n",
        "        # Extract window\n",
        "        window = ppg_signal[window_start:window_end]\n",
        "\n",
        "        # Skip if window is not full size or contains NaN\n",
        "        if len(window) != window_size or np.any(np.isnan(window)):\n",
        "            continue\n",
        "\n",
        "        # Count peaks in this window\n",
        "        peak_count = count_peaks_in_window(window, height_threshold=HEIGHT_THRESHOLD)\n",
        "\n",
        "        # Only keep if exactly 1 peak\n",
        "        if peak_count == 1:\n",
        "            windows.append(window)\n",
        "\n",
        "    return windows"
      ],
      "metadata": {
        "id": "uHyRuDUL3GZy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Peaks in Window"
      ],
      "metadata": {
        "id": "W85do-Er5mzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_peaks_in_window(window, height_threshold):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for i in range(1, len(window) - 1):\n",
        "        if window[i-1] < window[i] > window[i+1]:\n",
        "            if window[i] > height_threshold:\n",
        "                count += 1\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "id": "K9_65obD5gQq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Template"
      ],
      "metadata": {
        "id": "UxGkIAX05tmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the pseudocode by the paper, they calculate template using mean of 16 min window"
      ],
      "metadata": {
        "id": "emTW0N7LCnXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_template(windows):\n",
        "\n",
        "    if len(windows) == 0:\n",
        "        return None\n",
        "\n",
        "    template = np.mean(windows, axis=0)\n",
        "    return template"
      ],
      "metadata": {
        "id": "BCMmt8PS5gNI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity\n",
        "Computes Cosine Similarity between window and template"
      ],
      "metadata": {
        "id": "n1a-kR_w5wrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(window, template):\n",
        "\n",
        "    if template is None or len(window) != len(template):\n",
        "        return 0.0\n",
        "\n",
        "    # Using scipy's cosine distance: similarity = 1 - distance\n",
        "    similarity = 1 - cosine(window, template)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "8XZT_ENq5gKV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter windows by similarity\n",
        "Filters windows by cosine similairity to template"
      ],
      "metadata": {
        "id": "6n5HEhTB51IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_windows_by_similarity(windows, template, similarity_threshold):\n",
        "\n",
        "    filtered_windows = []\n",
        "\n",
        "    for window in windows:\n",
        "        similarity = cosine_similarity(window, template)\n",
        "\n",
        "        if similarity >= similarity_threshold:\n",
        "            filtered_windows.append(window)\n",
        "\n",
        "    return filtered_windows\n"
      ],
      "metadata": {
        "id": "BeP0mj3M5gH8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run All 1-Second Segmentation Process"
      ],
      "metadata": {
        "id": "pXQTMyMJ52Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_process_window(ppg_signal):\n",
        "\n",
        "    # Step 1: Detect peaks\n",
        "    peaks = detect_peaks(ppg_signal, HEIGHT_THRESHOLD, DISTANCE_THRESHOLD)\n",
        "\n",
        "    # Step 2: Extract windows centered on peaks\n",
        "    windows = extract_windows(ppg_signal, peaks, WINDOW_SIZE)\n",
        "\n",
        "    # Step 3: Compute template\n",
        "    template = compute_template(windows)\n",
        "\n",
        "    # Step 4: Filter by similarity\n",
        "    if template is not None:\n",
        "        filtered_windows = filter_windows_by_similarity(windows, template, SIMILARITY_THRESHOLD)\n",
        "    else:\n",
        "        filtered_windows = []\n",
        "\n",
        "    # Statistics\n",
        "    stats = {\n",
        "        'n_peaks': len(peaks),\n",
        "        'n_windows_extracted': len(windows),\n",
        "        'n_windows_filtered': len(filtered_windows),\n",
        "        'rejection_rate': 1 - (len(filtered_windows) / len(peaks)) if len(peaks) > 0 else 1.0\n",
        "    }\n",
        "\n",
        "    return filtered_windows, stats"
      ],
      "metadata": {
        "id": "Gopwovkc5gFg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Path(OUTPUT_PATH3).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ppg_files = sorted(Path(OUTPUT_PATH2).glob('case_*_filtered.npy'))\n",
        "print(f\"Found {len(ppg_files)} files to process\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for ppg_file in tqdm(ppg_files, desc=\"Processing files\"):\n",
        "    # Parse filename\n",
        "    parts = ppg_file.stem.split('_')\n",
        "    caseid = int(parts[1])\n",
        "    glucose_time = int(parts[3])\n",
        "\n",
        "    # Load 16-minute PPG window\n",
        "    try:\n",
        "        ppg_16min = np.load(ppg_file)\n",
        "    except (EOFError, ValueError, FileNotFoundError):\n",
        "        results.append({\n",
        "            'caseid': caseid,\n",
        "            'glucose_time': glucose_time,\n",
        "            'n_peaks': 0,\n",
        "            'n_windows_extracted': 0,\n",
        "            'n_windows_filtered': 0,\n",
        "            'rejection_rate': 1.0\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Process this 16-min window\n",
        "    filtered_windows, stats = main_process_window(ppg_16min)\n",
        "\n",
        "    # Save valid segments if any\n",
        "    if len(filtered_windows) > 0:\n",
        "        segments_array = np.array(filtered_windows)  # Shape: (n_segments, 100)\n",
        "        output_file = Path(OUTPUT_PATH3) / f'case_{caseid}_time_{glucose_time}_1s_segments.npy'\n",
        "        np.save(output_file, segments_array)\n",
        "\n",
        "    # Track results\n",
        "    results.append({\n",
        "        'caseid': caseid,\n",
        "        'glucose_time': glucose_time,\n",
        "        'n_peaks': stats['n_peaks'],\n",
        "        'n_windows_extracted': stats['n_windows_extracted'],\n",
        "        'n_windows_filtered': stats['n_windows_filtered'],\n",
        "        'rejection_rate': stats['rejection_rate']\n",
        "    })\n",
        "\n",
        "# Meta data\n",
        "summary_df = pd.DataFrame(results)\n",
        "summary_df.to_csv(Path(OUTPUT_PATH3).parent / '1s_seg_metadata.csv', index=False)"
      ],
      "metadata": {
        "id": "pJ42H8qD5gC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(filtered_windows).shape"
      ],
      "metadata": {
        "id": "1ieMlajY5gAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae21df6a-4188-414c-8cea-787f74579c23"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(791, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZzbPdV45fg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkU1J6Wm2lhO"
      },
      "outputs": [],
      "source": []
    }
  ]
}